#!/usr/bin/env just

@help:
    just --list

# local dev environment
init:
    pipenv install --dev

test:
    pipenv run pytest 

autolint:
    pipenv run ruff check --fix
    pipenv run ruff check --select I --fix .

@clean:
    -find . -name "*.pyc" -exec rm -f {} \;
    just _find_and_remove_dir ".*_cache"
    just _find_and_remove_dir "*.egg-info"
    just _find_and_remove_dir "dist"
    just _find_and_remove_dir "build"
    
@_find_and_remove_dir PATTERN:
    find . -name "{{PATTERN}}" -type d -maxdepth 1 -exec rm -rf {} \;

build-old:
    pipenv run python setup.py bdist_egg

# to run on the submit docker container
submit:
    #!/bin/bash
    echo "Preparing some dependencies until we have a better solution ..."
    pip install requests pymongo
    spark-submit \
      --deploy-mode client \
      --master spark://master:7077 \
      --py-files (ls dist/) \
      ./tweets/spark_job.py

# package the python to use with spark submit
build: 
    #!/bin/bash
    PIP_REQUIRE_VIRTUALENV=false
    BUILD_DIR=build
    CONDA_DIR=${BUILD_DIR}/conda
    mkdir -p ./${BUILD_DIR}
    rm -rf ${CONDA_DIR}
    
    PIPENV_VERBOSITY=-1 pipenv run python -c "import platform; print('.'.join(platform.python_version_tuple()[:2]))" > ${BUILD_DIR}/.python-version
    
    pipenv requirements > ${BUILD_DIR}/requirements.txt
    cat <<EOF > ${BUILD_DIR}/environment.yml
    dependencies:
        - python=$(cat ${BUILD_DIR}/.python-version)
        - pip:
            - -r requirements.txt
    EOF
    conda env create -p ${CONDA_DIR} -f ${BUILD_DIR}/environment.yml
    CURRENT_DIR=$(pwd)
    cd ${BUILD_DIR} && conda run -p ${CURRENT_DIR}/${CONDA_DIR} conda package --pkg-name thejob
    cd ${CURRENT_DIR}
    mkdir dist/
    mv ${BUILD_DIR}/thejob-*.tar.bz2 dist/